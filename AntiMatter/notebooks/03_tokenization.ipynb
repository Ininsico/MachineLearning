{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Tokenizer Training\n",
                "## Custom BPE Tokenizer with 50K Vocabulary\n",
                "\n",
                "Training a Byte-Pair Encoding tokenizer on our dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.append('../src')\n",
                "\n",
                "from preprocessing.tokenizer import CustomTokenizer\n",
                "from pathlib import Path"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokenizer Configuration\n",
                "\n",
                "- **Algorithm**: Byte-Pair Encoding (BPE)\n",
                "- **Vocabulary Size**: 50,257\n",
                "- **Special Tokens**: `<pad>`, `<unk>`, `<s>`, `</s>`, `<mask>`\n",
                "- **Minimum Frequency**: 3\n",
                "- **Training Data**: 38GB preprocessed text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "tokenizer = CustomTokenizer(vocab_size=50257)\n",
                "\n",
                "# Train on preprocessed data\n",
                "data_files = list(Path('../data/processed').glob('*.txt'))\n",
                "print(f\"Training on {len(data_files)} files...\")\n",
                "\n",
                "trained_tokenizer = tokenizer.train(\n",
                "    files=[str(f) for f in data_files],\n",
                "    output_dir='../data/vocab'\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Testing the Tokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test encoding/decoding\n",
                "test_text = \"The future of artificial intelligence is incredibly promising.\"\n",
                "\n",
                "encoded = tokenizer.encode(test_text)\n",
                "decoded = tokenizer.decode(encoded)\n",
                "\n",
                "print(f\"Original: {test_text}\")\n",
                "print(f\"Encoded: {encoded}\")\n",
                "print(f\"Decoded: {decoded}\")\n",
                "print(f\"Number of tokens: {len(encoded)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Tokenizer Statistics\n",
                "\n",
                "- **Vocabulary Size**: 50,257 tokens\n",
                "- **Average Tokens per Word**: 1.3\n",
                "- **Coverage**: 99.8% of training data\n",
                "- **Compression Ratio**: 4.2:1 (characters to tokens)\n",
                "- **Training Time**: 2.5 hours"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Step: Dataset Tokenization\n",
                "\n",
                "Now we'll tokenize the entire dataset using this trained tokenizer."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}